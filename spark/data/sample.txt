Apache Spark is a unified analytics engine for large-scale data processing.
It provides high-level APIs in Java, Scala, Python and R, and an optimized engine 
that supports general execution graphs.

Spark runs programs up to 100x faster than Hadoop MapReduce in memory, or 10x faster on disk.
It is designed to cover a wide range of workloads such as batch applications, iterative algorithms,
interactive queries and streaming.

The main features of Apache Spark include:

Speed: Run workloads 100x faster
Ease of Use: Write applications quickly in Java, Scala, Python, R, and SQL
Generality: Combine SQL, streaming, and complex analytics
Runs Everywhere: Spark runs on Hadoop, Apache Mesos, Kubernetes, standalone, or in the cloud

Apache Spark has several core components:

Spark Core: The foundation of the platform that provides distributed task dispatching, scheduling,
and basic I/O functionalities. Spark Core is the base engine for large-scale parallel and distributed
data processing.

Spark SQL: A module for working with structured data that allows querying data via SQL as well as
Apache Hive variant of SQL called HQL. It provides a programming abstraction called DataFrames
and can also act as distributed SQL query engine.

Spark Streaming: This component enables processing of live streams of data. Examples of data streams
could be log files generated by production web servers, or queues of messages containing status updates
posted by users of a web service.

MLlib: A distributed machine learning framework that provides multiple types of machine learning
algorithms including classification, regression, clustering, and collaborative filtering.

GraphX: A distributed graph-processing framework on top of Apache Spark for manipulating graphs
and performing graph-parallel computations.

Spark is used by organizations like Netflix, Yahoo, and eBay for big data processing.
It has become one of the most active Apache projects with over 1000 contributors from over 250 organizations.

The Spark ecosystem continues to grow with new capabilities and integrations being added regularly.
Whether you're processing batch data, streaming data, or building machine learning models,
Apache Spark provides the tools and APIs to get the job done efficiently.

This sample text file is designed to demonstrate word counting functionality in Apache Spark.
The word "Spark" appears multiple times throughout this document to show frequency analysis.
Data processing with Apache Spark is both powerful and flexible for various use cases.
