// Databricks notebook source
// MAGIC %md
// MAGIC # Word Count Example in Azure Databricks
// MAGIC 
// MAGIC This notebook demonstrates running a word count analysis using Spark in Azure Databricks.
// MAGIC 
// MAGIC ## Prerequisites
// MAGIC 1. Upload a sample text file to DBFS (e.g., `/FileStore/sample.txt`)
// MAGIC 2. Or use this notebook to create sample data
// MAGIC 
// MAGIC ## Steps
// MAGIC 1. Create sample data in DBFS
// MAGIC 2. Run word count analysis
// MAGIC 3. Display results

// COMMAND ----------

// MAGIC %md
// MAGIC ## Step 1: Create Sample Data

// COMMAND ----------

// Create sample text content
val sampleText = """
Apache Spark is a unified analytics engine for large-scale data processing.
It provides high-level APIs in Java, Scala, Python and R, and an optimized engine 
that supports general execution graphs.

Spark runs programs up to 100x faster than Hadoop MapReduce in memory, or 10x faster on disk.
It is designed to cover a wide range of workloads such as batch applications, iterative algorithms,
interactive queries and streaming.

The main features of Apache Spark include:

Speed: Run workloads 100x faster
Ease of Use: Write applications quickly in Java, Scala, Python, R, and SQL
Generality: Combine SQL, streaming, and complex analytics
Runs Everywhere: Spark runs on Hadoop, Apache Mesos, Kubernetes, standalone, or in the cloud

Apache Spark has several core components:

Spark Core: The foundation of the platform that provides distributed task dispatching, scheduling,
and basic I/O functionalities. Spark Core is the base engine for large-scale parallel and distributed
data processing.

Spark SQL: A module for working with structured data that allows querying data via SQL as well as
Apache Hive variant of SQL called HQL. It provides a programming abstraction called DataFrames
and can also act as distributed SQL query engine.

Spark Streaming: This component enables processing of live streams of data. Examples of data streams
could be log files generated by production web servers, or queues of messages containing status updates
posted by users of a web service.

MLlib: A distributed machine learning framework that provides multiple types of machine learning
algorithms including classification, regression, clustering, and collaborative filtering.

GraphX: A distributed graph-processing framework on top of Apache Spark for manipulating graphs
and performing graph-parallel computations.

Spark is used by organizations like Netflix, Yahoo, and eBay for big data processing.
It has become one of the most active Apache projects with over 1000 contributors from over 250 organizations.

The Spark ecosystem continues to grow with new capabilities and integrations being added regularly.
Whether you're processing batch data, streaming data, or building machine learning models,
Apache Spark provides the tools and APIs to get the job done efficiently.

This sample text file is designed to demonstrate word counting functionality in Apache Spark.
The word "Spark" appears multiple times throughout this document to show frequency analysis.
Data processing with Apache Spark is both powerful and flexible for various use cases.
"""

// Write to DBFS
dbutils.fs.put("dbfs:/FileStore/sample.txt", sampleText, true)
println("Sample file created at: dbfs:/FileStore/sample.txt")

// COMMAND ----------

// MAGIC %md
// MAGIC ## Step 2: Run Word Count Analysis (Scala Version)

// COMMAND ----------

// Read the file - Note: SparkContext (sc) is already available in Databricks
val inputFile = "dbfs:/FileStore/sample.txt"
println(s"Reading file: $inputFile")

// Read the input file
val lines = sc.textFile(inputFile)

// Split lines into words and process
val words = lines
  .flatMap(line => line.split("\\s+"))
  .filter(_.nonEmpty)
  .map(word => word.toLowerCase.replaceAll("[^a-z0-9]", ""))
  .filter(_.nonEmpty)

// Count word occurrences
val wordCounts = words
  .map(word => (word, 1))
  .reduceByKey(_ + _)

// Sort by count descending and get top 20
val topWords = wordCounts
  .map { case (word, count) => (count, word) }
  .sortByKey(ascending = false)
  .map { case (count, word) => (word, count) }
  .take(20)

// Display results
println("\n" + "=" * 50)
println("Top 20 most frequent words:")
println("=" * 50)

topWords.zipWithIndex.foreach { case ((word, count), index) =>
  println(f"${index + 1}%2d. $word%-20s : $count%d")
}

// Statistics
val totalWords = words.count()
val uniqueWords = wordCounts.count()

println("\n" + "=" * 50)
println("Statistics:")
println("=" * 50)
println(s"Total words: $totalWords")
println(s"Unique words: $uniqueWords")
println("=" * 50)

// COMMAND ----------

// MAGIC %md
// MAGIC ## Step 3: Visualize Results with DataFrame API

// COMMAND ----------

// Convert to DataFrame for better visualization
val wordCountsDF = wordCounts
  .map { case (word, count) => (word, count) }
  .toDF("word", "count")
  .orderBy($"count".desc)

display(wordCountsDF.limit(20))

// COMMAND ----------

// MAGIC %md
// MAGIC ## Step 4: Python Version (Alternative)

// COMMAND ----------

// MAGIC %python
// MAGIC # Read the file - SparkContext (sc) and SparkSession (spark) are pre-configured
// MAGIC input_file = "dbfs:/FileStore/sample.txt"
// MAGIC print(f"Reading file: {input_file}")
// MAGIC 
// MAGIC # Read the input file
// MAGIC lines = sc.textFile(input_file)
// MAGIC 
// MAGIC # Process the text
// MAGIC import re
// MAGIC 
// MAGIC words = (lines
// MAGIC     .flatMap(lambda line: line.split())
// MAGIC     .filter(lambda word: len(word) > 0)
// MAGIC     .map(lambda word: re.sub(r'[^a-z0-9]', '', word.lower()))
// MAGIC     .filter(lambda word: len(word) > 0))
// MAGIC 
// MAGIC # Count occurrences
// MAGIC word_counts = (words
// MAGIC     .map(lambda word: (word, 1))
// MAGIC     .reduceByKey(lambda a, b: a + b))
// MAGIC 
// MAGIC # Sort and get top 20
// MAGIC top_words = (word_counts
// MAGIC     .map(lambda pair: (pair[1], pair[0]))
// MAGIC     .sortByKey(ascending=False)
// MAGIC     .map(lambda pair: (pair[1], pair[0]))
// MAGIC     .take(20))
// MAGIC 
// MAGIC # Display results
// MAGIC print("\n" + "=" * 50)
// MAGIC print("Top 20 most frequent words:")
// MAGIC print("=" * 50)
// MAGIC 
// MAGIC for i, (word, count) in enumerate(top_words, 1):
// MAGIC     print(f"{i:2d}. {word:20s} : {count}")
// MAGIC 
// MAGIC # Statistics
// MAGIC total_words = words.count()
// MAGIC unique_words = word_counts.count()
// MAGIC 
// MAGIC print("\n" + "=" * 50)
// MAGIC print("Statistics:")
// MAGIC print("=" * 50)
// MAGIC print(f"Total words: {total_words}")
// MAGIC print(f"Unique words: {unique_words}")
// MAGIC print("=" * 50)

// COMMAND ----------

// MAGIC %md
// MAGIC ## Using with Uploaded JAR
// MAGIC 
// MAGIC If you want to use the compiled Java JAR in Databricks:
// MAGIC 
// MAGIC ### Option 1: Run via Notebook Cell
// MAGIC ```scala
// MAGIC %scala
// MAGIC // Assuming JAR is uploaded to DBFS
// MAGIC val jarPath = "dbfs:/FileStore/jars/spark-local-example-1.0.0.jar"
// MAGIC 
// MAGIC // Add JAR to SparkContext
// MAGIC sc.addJar(jarPath)
// MAGIC 
// MAGIC // Run the main class (if designed for programmatic invocation)
// MAGIC // Or use spark-submit via %sh
// MAGIC ```
// MAGIC 
// MAGIC ### Option 2: Create a Databricks Job
// MAGIC 1. Upload JAR to DBFS: `/FileStore/jars/spark-local-example-1.0.0.jar`
// MAGIC 2. Create a new Job in Databricks
// MAGIC 3. Set Main Class: `com.example.spark.WordCountAppDatabricks`
// MAGIC 4. Add Parameters: `["dbfs:/FileStore/sample.txt"]`
// MAGIC 5. Select cluster and run
// MAGIC 
// MAGIC ### Option 3: Use spark-submit in notebook
// MAGIC ```python
// MAGIC %sh
// MAGIC /databricks/spark/bin/spark-submit \
// MAGIC   --class com.example.spark.WordCountAppDatabricks \
// MAGIC   --master local[*] \
// MAGIC   dbfs:/FileStore/jars/spark-local-example-1.0.0.jar \
// MAGIC   dbfs:/FileStore/sample.txt
// MAGIC ```
